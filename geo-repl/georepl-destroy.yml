#!/usr/bin/ansible-playbook -vvv
---
- hosts: localhost #masters[0]  #TODO: test using masters[0]
  connection: local
  vars:
    slave_openshift_hostname: 10.180.26.100
    slave_openshift_token: 'eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJnbHVzdGVyZnMiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoiZ2VvcmVwbC10b2tlbi1nY2NsaCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJnZW9yZXBsIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiNDhiMzE0MDctZmU0YS0xMWU4LWJmMzgtMDAwZDNhNGYwMTM2Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmdsdXN0ZXJmczpnZW9yZXBsIn0.gXadbgZkwLtOTex4tcQjGII4npGySZw7sLhRenEXA1cQfgBbM4jjkkcbabms6HVzmBB6Z1iN8Lv2qPSrlyyQKgsLeAFmkwOHKZM7zBGWJxEOum3TSFJDu9b5zonsIHBLiNvITCUVlxZhmvfo9NvQlfJR_CIzKNeZdB-jpH2bOu2_sKrbXCvm-E1lTypm7LmnsHvcXsQ7p1d97-EBe2GUc1Aesle0s6_XV5yaEsUVuCwZlyX2aCQIjZFOEaHDmIW2FznWQdSeff7sUvZEVMQUQOcwQlOIot0FL7u00YsL7NJQwVDng_rRDORBmlm1AXiPnyklJA_ehpcgpt-BXWElbw'
  tasks:
  - name: OpenShift Login of master cluster
    shell: "oc project glusterfs"
  - shell: "oc config current-context"
    register: openshift_master_context

  - name: Sync all PVCs from Master to Slave cluster
    block:
    - name: OpenShift Login of slave cluster
      shell: "NO_PROXY={{ slave_openshift_hostname }} oc login --token={{ slave_openshift_token }} https://{{ slave_openshift_hostname }}:443 --insecure-skip-tls-verify && NO_PROXY={{ slave_openshift_hostname }} oc project glusterfs" #TODO: beautify
    - shell: "oc config current-context"
      register: openshift_slave_context
  
    - name: Register OpenShift Login credentials
      set_fact:
        oc_master: "oc --context={{ openshift_master_context.stdout }}"
        oc_slave:  "NO_PROXY={{ slave_openshift_hostname }} oc --context={{ openshift_slave_context.stdout  }}"
  
    - name: Deleting geo-replication on all PVCs
      shell: |
        set -x
        set +e
        SLAVE_ENDPOINT=`{{ oc_slave }} -n glusterfs get ep/heketi-db-storage-endpoints -o jsonpath='{.subsets[0].addresses[0].ip}'`
        GLUSTER_POD=`{{ oc_master }} -n glusterfs get po -o custom-columns=name:.metadata.name -l glusterfs=storage-pod --no-headers |head -n1`
        {{ oc_slave }} observe pvc --all-namespaces --once \
          -a '{ .spec.volumeName }' \
          -a '{ .metadata.annotations.volume\.beta\.kubernetes\.io\/storage-provisioner }' \
        |while read IGNORE1 IGNORE2 IGNORE3 IGNORE4 IGNORE5 PROJECT PVC SLAVE_PV STORAGECLASS
        do
          echo "Checking PVC '$PROJECT/$PVC = $SLAVE_PV' ($STORAGECLASS)"
          # Check if PVC is bound to PV
          [ "$SLAVE_PV"  == ""     ] && continue  #TODO: beautify
          [ "$SLAVE_PV"  == "\"\"" ] && continue
          [ "$STORAGECLASS" != "kubernetes.io/glusterfs" ] && continue

          MASTER_PV=`{{ oc_master }} -n $PROJECT get pvc/$PVC -o jsonpath={.spec.volumeName}`
          [ "$MASTER_PV" == ""     ] && continue  #Ignore PVC that exist on Slave side, but not on Master side

          echo "Slave PVC '$PROJECT/$PVC = $SLAVE_PV' exists. Destroying Geo-Replication."
  
          # Get gluster volume ids
           SLAVE_VOLUME=`{{ oc_slave }}  get pv/$SLAVE_PV  -o jsonpath={.spec.glusterfs.path}` 
          MASTER_VOLUME=`{{ oc_master }} get pv/$MASTER_PV -o jsonpath={.spec.glusterfs.path}` 
  
          # Setup Gluster-Gluster Geo-Replication
          GEO_SETUP1="{{ oc_master }} -n glusterfs exec $GLUSTER_POD gluster volume geo-replication $MASTER_VOLUME "
          GEO_SETUP2="$GEO_SETUP1 ssh://$SLAVE_ENDPOINT::$SLAVE_VOLUME "
          $GEO_SETUP1 status >/dev/null 2>&1
          [ "$?" == "0" ] && $GEO_SETUP2 stop && $GEO_SETUP2 delete
          {{ oc_slave }} -n $PROJECT delete pvc/$PVC
        done
    always: 
    - name: Reset OpenShift login back to master cluster
      shell: "oc config use-context {{ openshift_master_context.stdout }}"
